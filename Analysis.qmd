---
title: "Project Code V2"
format: html
editor: visual
---

## Agglomerative Hierarchical Clustering

```{r Scale variables}
# We've created variables based on user interactions in the duodata tibble for each user. Here we remove the user since it's non-numeric so we can create clusters
engagement_metrics <- duodata |>
  dplyr::select(-user)

set.seed(1234)

engagement_metrics_scaled <- engagement_metrics |> scale()
```

```{r PCA}
pca <- PCA(engagement_metrics, ncp = 6, scale.unit = TRUE)
var <- get_pca_var(pca)
```

```{r Show correlation matrix}
corrplot(var$coord, method = "color", 
         tl.col = "black", # Text label color
         tl.srt = 45, # Rotate text labels
         tl.cex = 0.6, # Text label size
         addCoef.col = "black", # Color of the correlation coefficient
         number.cex = 0.7, # Size of the correlation coefficient
         cl.ratio = 0.2, # Ratio of the color legend size
         cl.cex = 0.7, # Size of the color legend text
         cl.align.text = "r")# Align the text of the color legend to the right
```

```{r Scree Plot}
fviz_eig(pca, addlabels = TRUE)
```

```{r Clustering w PCA dimensions}
pca_dist_matrix <- dist(pca$ind$coord, method = 'euclidean')
pca_dims_hc <- hclust(pca_dist_matrix, method = "ward.D")

plot(pca_dims_hc)

k = 5

hc_dend <- as.dendrogram(pca_dims_hc)|> 
  color_labels(k = k)  |> 
  color_branches(k = k)

hc_dend |> 
  set("labels_cex", 0.5) |> 
  plot(main = "User Engagement Clusters PCA")

hc_cut <- cutree(pca_dims_hc, k = k)

sil_scores <- silhouette(hc_cut, dist = pca_dist_matrix)
fviz_silhouette(sil_scores)
```

```{r Agglomerative hierarchical clustering}
dist_matrix <- dist(engagement_metrics_scaled, method = "euclidean")
# Ward's method attempts to create clusters that are more evenly sized. Not sure the tradeoffs here. Analyzes the variance of clusters instead of distance directly.
hc <- hclust(dist_matrix, method = "ward.D")
  
k = 4 

# Seems like there may be 2 main clusters?
hc_dend <- as.dendrogram(hc)|> 
  color_labels(k = k)  |> 
  color_branches(k = k)

hc_dend |> 
  set("labels_cex", 0.5) |> 
  plot(main = "User Engagement Clusters")



# take the cluster and assign it back to the variable... is this possible??
# object I get should have the labels
# Figure out what to do next with the clusters
```

```{r Cut tree into clusters}
hc_cut <- cutree(hc, k = k)

sil_scores <- silhouette(hc_cut, dist = dist_matrix)
fviz_silhouette(sil_scores)

engagement_metrics$hc_cluster <- hc_cut

ggplot(engagement_metrics, aes(x = as.factor(hc_cluster), fill = as.factor(hc_cluster))) +
  geom_bar() +
  labs(title = "Number of Users in Each Cluster",
       x = "Cluster",
       y = "Number of Users",
       fill = "Clusters") +
  theme_minimal()
```

```{r plot average engagement_metrics per cluster}
cluster_averages <- engagement_metrics |>
  group_by(hc_cluster) |>
  summarize(across(where(is.numeric), mean)) |>
  ungroup() |>
  pivot_longer(-hc_cluster, names_to = "engagement_metrics", values_to = "value")

ggplot(cluster_averages, aes(x = engagement_metrics, y = value, fill = engagement_metrics)) + 
  geom_col() +
  geom_text(aes(label = round(value, 2)), position = position_dodge(width = 0.9), hjust = -0.2, vjust = 0.5) +
  coord_flip() +
  facet_wrap(~ hc_cluster) +
  theme_minimal() + 
  labs(x = "Engagement Metric", y = "Value", title = "Engagement Metrics by Cluster") +
  theme(legend.position = "none")
```

```{r}
set.seed(1234) 

filtered_data <- duodata |>
  dplyr::select(-avg_days)

train <- sample_frac(filtered_data, 0.75)
test <- filter(filtered_data, !duodata$user %in% 
                 train$user)
```

```{r XGBoost}
library(xgboost)

filtered_data <- filtered_data |> dplyr::select(-user)

features <- filtered_data[, setdiff(names(filtered_data), "drop_day")]
labels <- filtered_data$drop_day

features_matrix <- as.matrix(features)
labels_matrix <- as.matrix(labels)

set.seed(1234)
dtrain <- xgb.DMatrix(data = features_matrix, label = labels_matrix)

# Define parameters
params <- list(
  booster = "gbtree",
  eta = 0.3,
  max_depth = 6,
  objective = "reg:squarederror" # Use "reg:squarederror" for regression tasks
)

nrounds <- 100

# Train the model
xgb_model <- xgb.train(params = params, data = dtrain, nrounds = nrounds)

```

```{r}
dtest <- xgb.DMatrix(data = features_matrix, label = labels_matrix)

predictions <- predict(xgb_model, dtest)

rmse <- sqrt(mean((predictions - labels)^2))
print(rmse)
```

```{r}
importance_matrix <- xgb.importance(feature_names = colnames(dtrain), model = xgb_model)
xgb.plot.importance(importance_matrix)
```

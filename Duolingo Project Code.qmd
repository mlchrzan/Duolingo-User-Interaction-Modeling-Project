---
title: "Duolingo Project Code"
author: "Michael Chrzan, Alexa Sparks, Tracy Li"
format: html
editor: visual
---

# Introduction

...

# Research Questions

...

# Setup

## Libraries

Which libraries will we use and why?

```{r libraries}
library(tidyverse)
library(skimr)
library(naniar)
library(gt)
library(MASS)
library(caret)
library(vip)
library(rpart.plot)
library(randomForest)
library(FactoMineR)
library(factoextra)
library(corrplot)
library(mclust)
library(DescTools)
library(stats)
library(dendextend)
library(ggrepel)
library(rgl)
```

## Import Data

```{r import-data}
duodata <- read_csv("data_flatformat.csv")
```

# Explore Data

```{r view-data}
#glimpse(duodata)
duoskim <- skim(duodata)
duoskim
```

## Original Missing Data

```{r viz-missing}
#Setting sample since these packaged functions throw errors for large data
set.seed(1234)
duosample <- sample_n(duodata, 2622956/54)

vis_miss(duosample, 
         show_perc = TRUE) 
miss_var_summary(duosample)

duosample_mis <- duosample |> 
  dplyr::select(prompt, time, client, format, session)

gg_miss_var(duosample_mis)
gg_miss_var(duosample_mis, 
            facet = client, 
            show_pct = TRUE)
gg_miss_var(duosample_mis, 
            facet = format, 
            show_pct = TRUE)
gg_miss_var(duosample_mis, 
            facet = session, 
            show_pct = TRUE)
gg_miss_upset(duosample_mis)

```

```{r examine-mis-prompt}
#Examining Missiningness Related to Prompt and Time with Other Variables #Is all the missing prompt data in format? Appears so (NOTE, not the sample now, full dataset) 
#Prompt 
duodata |> 
  dplyr::select(prompt, format, session, client) |> 
  group_by(format, session, client) |> 
  miss_var_summary() |> 
  arrange(desc(pct_miss)) 

#Time 
duodata |> 
  dplyr::select(time, format, session, client) |> 
  group_by(format, session, client) |> 
  miss_var_summary() |> 
  arrange(desc(pct_miss))
```

```{r examine-session-format-distribution}

duodata |> 
  group_by(session, format) |> 
  summarize(n = n())

#NOTE: NOT evenly distributed formats within session types. Reverse translate by far the most common
```

# Data Cleaning

```{r remove-mis-remaining}
duodata <- duodata |> 
  dplyr::select(-prompt) |> 
  na.omit()
```

```{r viz-variable-dist}
duodata |> 
  pivot_longer(cols = c(part_of_speech, client, session, format), 
               names_to = "variable", 
               values_to = "value") |> 
  ggplot() + geom_bar(aes(value)) + 
  facet_wrap(. ~ variable, scales = "free") + 
  coord_flip() 

duodata |> 
  pivot_longer(cols = c(dependency_label, countries), 
               names_to = "variable", 
               values_to = "value") |> 
  ggplot() + 
  geom_bar(aes(value)) + 
  facet_wrap(. ~ variable, scales = "free") + 
  coord_flip() 

duodata |> 
  pivot_longer(cols = where(is.numeric), 
               names_to = "variable", 
               values_to = "value") |> 
  ggplot() + 
  geom_histogram(aes(value)) + 
  facet_wrap(. ~ variable, scales = "free") 

#Should we include folks at 0 days?

```

```{r check-data}
distinct(duodata, countries) 

#Checking Time Values 
summary(duodata$time)
duodata |> 
  ggplot(aes(x = time)) + 
  geom_histogram()

  #Checking average user time
duodata |> 
  group_by(user) |> 
  mutate(avg_time = mean(time)) |> 
  dplyr::select(user, avg_time, time) |> 
  arrange(desc(time))
      #QTNA: Why don't some users have an avg_time but have time? #Number of Users with ONLY        less than 1 day, 52 - resulting in ~30000 rows 

  #This shows that 60 users used the app for longer than 14 hours in one session
duodata |> 
  group_by(user) |> 
  summarize(max_time = max(time)) |> 
  arrange(desc(max_time)) |> 
  filter(max_time < 50400)


#Large Dependency Edge Head Value (originally 442 rows larger than 10)
summary(duodata$dependency_edge_head)
duodata |> 
  ggplot(aes(x = dependency_edge_head)) + 
  geom_histogram()



#Users who used the app less than 1 day
one_days <- duodata |> 
  group_by(user) |> 
  mutate(max_days = max(days)) |> 
  ungroup() |> 
  filter(max_days < 1) 

n_distinct(one_days$user) 
#Curious if there's any patterns in how these folks use it rather than others. 
#Michael noticed from the histogram that users trail off as days increases.

#Proportion of Correct Responses, 14.62% 
sum(duodata$label == 1)/sum(duodata$label == 0)

```

```{r clean-data}
#Removing instances with logging issues (time < 1) as noted in the documentation and time > 3 hours for reasonability
duodata <- duodata |> 
  filter(time > 0) |> 
  filter(time < 10800)
```

```{r variable-creation}
#AVERAGES

#Averge Accuracy, Session Time, and Days for each User for their entire time 
duodata <- duodata |> 
  group_by(user) |> 
  mutate(avg_total_accuracy = mean(label), 
         avg_session_time = mean(time), 
         avg_days = mean(days)) |> 
  ungroup()
  #dplyr::select(user, session_id, exercise_id, contains('avg')) 


#Averge Number of Exercises per Session (get and remove Number of Exercises by Session_id)
duodata <- duodata |> 
  group_by(session_id) |> 
  mutate(num_exercises_this_session = n_distinct(exercise_index)) |> 
  ungroup() |> 
  group_by(user) |>
  mutate(avg_exercises_per_session = mean(num_exercises_this_session)) |>
  ungroup() |> 
  dplyr::select(-num_exercises_this_session)
  #dplyr::select(user, session_id, num_session_exercises, avg_exer_per_session) 


#Averge Number of Sessions per Day (get and remove Number of Sessions By Day) 
duodata <- duodata |> 
  mutate(days_count = floor(days)) |> 
  group_by(user, days_count) |> 
  mutate(num_sessions_user_in_day = n_distinct(session_id)) |> 
  ungroup() |> 
  group_by(user) |> 
  mutate(avg_num_sessions_in_day = mean(num_sessions_user_in_day)) |> 
  dplyr::select(-days_count, -num_sessions_user_in_day)
  #dplyr::select(user, days_count, session_id, num_sessions_this_day, avg_num_sessions) 


#Average Exercise Accuracy by User 
user_ex_acc <- duodata |> 
  group_by(user, exercise_id) |> 
  summarize(avg_accuracy_by_exercise = mean(label), 
            .groups = 'keep') |>
  ungroup() |> 
  dplyr::select(-exercise_id) |>
  group_by(user) |> 
  summarize(avg_exercise_accuracy = mean(avg_accuracy_by_exercise), 
            .groups = 'keep') |> 
  ungroup()

duodata <- left_join(duodata, user_ex_acc, 
                     by = join_by(user))

rm(user_ex_acc)


#Average Overall Session Accuracy
user_sess_acc <- duodata |> 
  group_by(user, session_id) |> 
  summarize(avg_accuracy_by_session = mean(label), 
            .groups = 'keep') |>
  ungroup() |> 
  dplyr::select(-session_id) |>
  group_by(user) |> 
  summarize(avg_session_accuracy = mean(avg_accuracy_by_session), 
            .groups = 'keep') |> 
  ungroup()

duodata <- left_join(duodata, user_sess_acc, 
                     by = join_by(user))

rm(user_sess_acc)

  
#Average Session Accuracy by Type
  #Average Test Accuracy
user_test_acc <- duodata |> 
  filter(session == 'test') |> 
  group_by(user) |> 
  summarize(avg_test_accuracy = mean(label), 
            .groups = 'keep') |> 
  ungroup()

duodata <- left_join(duodata, user_test_acc, 
                     by = join_by(user)) |>
  mutate(avg_test_accuracy = if_else(is.na(avg_test_accuracy),
                                           avg_session_accuracy,
                                           avg_test_accuracy))

rm(user_test_acc)

  #Average Lesson Accuracy
user_lesson_acc <- duodata |> 
  filter(session == 'lesson') |> 
  group_by(user) |> 
  summarize(avg_lesson_accuracy = mean(label), 
            .groups = 'keep') |> 
  ungroup()

duodata <- left_join(duodata, user_lesson_acc, 
          by = join_by(user)) |>
  mutate(avg_lesson_accuracy = if_else(is.na(avg_lesson_accuracy),
                                             avg_session_accuracy,
                                             avg_lesson_accuracy))

rm(user_lesson_acc)

  #Average Practice Accuracy
user_practice_acc <- duodata |> 
  filter(session == 'practice') |> 
  group_by(user) |> 
  summarize(avg_practice_accuracy = mean(label), 
            .groups = 'keep') |> 
  ungroup()

duodata <- left_join(duodata, user_practice_acc, 
          by = join_by(user)) |>
  mutate(avg_practice_accuracy = if_else(is.na(avg_practice_accuracy),
                                               avg_session_accuracy,
                                               avg_practice_accuracy))

rm(user_practice_acc)

#Average Accuracy per User by Format
  #Reverse_Translate
user_rever_t_acc <- duodata |> 
  filter(format == 'reverse_translate') |> 
  group_by(user) |> 
  summarize(avg_reverse_trans_accuracy = mean(label), 
            .groups = 'keep') |> 
  ungroup()
  
duodata <- left_join(duodata, user_rever_t_acc, 
          by = join_by(user)) |>
  mutate(avg_reverse_trans_accuracy = if_else(is.na(avg_reverse_trans_accuracy),
                                                    avg_session_accuracy,
                                                    avg_reverse_trans_accuracy))

rm(user_rever_t_acc)

  #Reverse_Tap
user_rever_tap_acc <- duodata |> 
  filter(format == 'reverse_tap') |> 
  group_by(user) |> 
  summarize(avg_reverse_tap_accuracy = mean(label), 
            .groups = 'keep') |> 
  ungroup()

duodata <- left_join(duodata, user_rever_tap_acc, 
                     by = join_by(user)) |>
  mutate(avg_reverse_tap_accuracy = if_else(is.na(avg_reverse_tap_accuracy),
                                                  avg_session_accuracy,
                                                  avg_reverse_tap_accuracy))

rm(user_rever_tap_acc)

  #Listen
user_listen_acc <- duodata |> 
  filter(format == 'listen') |> 
  group_by(user) |> 
  summarize(avg_listen_accuracy = mean(label), 
            .groups = 'keep') |> 
  ungroup()
  
duodata <- left_join(duodata, user_listen_acc, 
                     by = join_by(user)) |>
  mutate(avg_listen_accuracy = if_else(is.na(avg_listen_accuracy),
                                       avg_session_accuracy,
                                       avg_listen_accuracy))

rm(user_listen_acc)
  


#TOTALS


#Total Number of of Countries by User 
duodata <- duodata |> 
  group_by(user) |> 
  mutate(country_count = str_count(countries, ",") + 1) |> 
  ungroup() 
  #dplyr::select(countries, country_count) |> #arrange(desc(country_count)) 


#Total Time Spent on App by User
user_times <- duodata |> 
  group_by(user, exercise_id) |> 
  summarize(total_time_exer = sum(time), 
            .groups = 'keep') |> 
  ungroup() |> 
  group_by(user) |> 
  summarise(total_time = mean(total_time_exer), 
            .groups = 'keep')

duodata <- left_join(duodata, user_times, by = join_by(user))
rm(user_times)


#Total Number of Exercises per User
tot_exer <- duodata |> 
  group_by(user) |> 
  summarize(total_exercises = n_distinct(exercise_id)) |> 
  ungroup()

duodata <- left_join(duodata, tot_exer, by = join_by(user))
rm(tot_exer)


#Total Number of Sessions per User
tot_sess <-  duodata |> 
  group_by(user) |> 
  summarize(total_sessions = n_distinct(session_id)) |> 
  ungroup()

duodata <- left_join(duodata, tot_sess, by = join_by(user))
rm(tot_sess)


#Total Number of Session by Type per User
  #Test
n_test <- duodata |> 
  filter(session == 'test') |> 
  group_by(user) |>
  summarize(total_test_sessions = n_distinct(session_id)) |> 
  ungroup() 

duodata <- left_join(duodata, n_test, by = join_by(user)) |> 
  mutate(total_test_sessions = if_else(is.na(total_test_sessions), 0, total_test_sessions))

rm(n_test)

  #Practice
n_practice <- duodata |> 
  filter(session == 'practice') |> 
  group_by(user) |>
  summarize(total_practice_sessions = n_distinct(session_id)) |> 
  ungroup() 

duodata <- left_join(duodata, n_practice, by = join_by(user)) |> 
  mutate(total_practice_sessions = if_else(is.na(total_practice_sessions), 0, total_practice_sessions))

rm(n_practice)

  #Lesson
n_lesson <- duodata |> 
  filter(session == 'lesson') |> 
  group_by(user) |>
  summarize(total_lesson_sessions = n_distinct(session_id)) |> 
  ungroup() 

duodata <- left_join(duodata, n_lesson, by = join_by(user)) |> 
  mutate(total_lesson_sessions = if_else(is.na(total_lesson_sessions), 0, total_lesson_sessions))

rm(n_lesson)


#Total Number of Each Format per User
  #Reverse Translate
n_rever_t <- duodata |> 
  filter(format == 'reverse_translate') |> 
  group_by(user) |>
  summarize(total_reverse_translate = n_distinct(session_id)) |> 
  ungroup()

duodata <- left_join(duodata, n_rever_t, by = join_by(user))  |> 
  mutate(total_reverse_translate = if_else(is.na(total_reverse_translate), 0, total_reverse_translate))

rm(n_rever_t)

  #Reverse_tap
n_rever_tap <- duodata |> 
  filter(format == 'reverse_tap') |> 
  group_by(user) |>
  summarize(total_reverse_tap = n_distinct(session_id)) |> 
  ungroup()

duodata <- left_join(duodata, n_rever_tap, by = join_by(user)) |> 
  mutate(total_reverse_tap = if_else(is.na(total_reverse_tap), 0, total_reverse_tap))

rm(n_rever_tap)

  #Listen
n_listen <- duodata |> 
  filter(format == 'listen') |> 
  group_by(user) |>
  summarize(total_listen = n_distinct(session_id)) |> 
  ungroup() 

duodata <- left_join(duodata, n_listen, by = join_by(user)) |> 
  mutate(total_listen = if_else(is.na(total_listen), 0, total_listen))

rm(n_listen)


  
#OTHER


#Get Number of Days Skipped by User
skipped <- duodata |>
  mutate(days_round_down = floor(days)) |> 
  group_by(user, days_round_down) |> 
  summarise(.groups = 'keep') |>
  ungroup() |> 
  mutate(days_skipped_user = days_round_down - dplyr::lag(days_round_down) - 1) |> 
  ungroup() |> 
  mutate(days_skipped_user = if_else(is.na(days_skipped_user) | days_skipped_user < 0, 
                                     0, 
                                     days_skipped_user))

duodata <- left_join(duodata |> mutate(days_round_down = floor(days)), 
          skipped, 
          by = join_by(user, days_round_down)) |> 
  dplyr::select(-days_round_down) 

rm(skipped)

#Get Last Day User Used App
duodata <- duodata |> 
  group_by(user) |> 
  mutate(drop_day = max(floor(days))) |> 
  ungroup()

#Accuracy on their last day
duodata <- duodata |> 
  filter(floor(days) == drop_day) |>
  group_by(user) |> 
  mutate(last_day_accuracy = mean(label)) |> 
  ungroup()
```

```{r restructure-data}
#Removing Variables not included in our final analysis 
duodata <- duodata |> dplyr::select(-countries, 
                                    -token,
                                    -part_of_speech,
                                    -dependency_label,
                                    -dependency_edge_head,
                                    -token_index,
                                    -instance_id,
                                    -exercise_id, 
                                    -days, 
                                    -exercise_index, 
                                    -client, 
                                    -session, 
                                    -format, 
                                    -time, 
                                    -session_id,
                                    -label)


#Check New Unit of Analysis (should be 2585 rows)
distinct(duodata)

#Removing Duplicated Rows (each row is still based on the tokens, so there are duplicates)
duodata <- distinct(duodata)

#Find duplicated columns
duplicated_columns <- duplicated(as.list(duodata))

#Show the names of duplicated columns
#colnames(duodata[duplicated_columns])

#Remove the duplicated columns
duodata <- duodata[, !duplicated_columns]
```

```{r check-new-distributions}
duodata |> 
  pivot_longer(cols = where(is.numeric), 
               names_to = "variable", 
               values_to = "value") |> 
  ggplot() + 
  geom_histogram(aes(value)) + 
  facet_wrap(. ~ variable, scales = "free") 
```

# Examine Introduced Missing Data

```{r new-missing}
#NOTE: No longer sampling due to reduced size of dataset from aggregation
vis_miss(duodata, 
         show_perc = TRUE) 
miss_var_summary(duodata)
#No Missing Data! 
```

# Explore Data

```{r data-viz-for-stories}
duodata |> 
  ggplot(aes(x = drop_day)) + 
  geom_histogram(fill = "#7ac70c") + 
  labs(title = 'User Retention Over Time',
       subtitle = 'Number of Users Returning to Duolingo Each Day Decreases Over Time', 
       x = 'Days', 
       y = 'Number of Users Lost') +
  theme_minimal()

duodata |> 
  ggplot(aes(x = avg_days)) + 
  geom_histogram(fill = "#7ac70c") + 
  labs(title = 'Users Average Number of Days',
       subtitle = 'As time progresses, less and less users return to the app', 
       x = 'Days', 
       y = 'Number of Users') +
  scale_y_continuous(labels = ~ format(.x, scientific = FALSE)) +
  theme_minimal() 

duodata |> 
  #filter(days_skipped_user > 0) |> 
  ggplot(aes(x = days_skipped_user)) + 
  geom_histogram(fill = "#7ac70c") + 
  labs(title = 'Number of Days Users Skipped',
       subtitle = 'Most users never skip a day', 
       x = 'Number of Days Skipped', 
       y = 'Number of Users') +
  scale_y_continuous(labels = ~ format(.x, scientific = FALSE)) +
  theme_minimal() 
```

# Dimension Reduction

```{r standardize}
duodata <- duodata |> 
  mutate(across(where(is.numeric), scale),
         across(where(is.numeric), as.vector))
```

```{r pca}
duo_pca <- PCA(dplyr::select(duodata, where(is.numeric)), ncp = 10)
```

```{r examine-PCA}
fviz_eig(duo_pca, addlabels = TRUE)

var <- get_pca_var(duo_pca)

corrplot(var$cor, 
         tl.col = "black", 
         method = "color")

corrplot(var$cos2, 
         is.corr = FALSE, 
         tl.col = "black", 
         method = "color")
```

# Create Clusters

## Hierarchical 

```{r hierarchical}
duo_diff <- dist(duodata, method = "euclidean")
duo_hc_c <- hclust(duo_diff, method = "ward.D")

duo_dend <- as.dendrogram(duo_hc_c)|> 
  color_labels(k = 3)  |> 
  color_branches(k = 3)

duo_dend |> 
  set("labels_cex", 0.5) |> 
  plot(main = "User Engagement Clusters")
```

## GMM

```{r build-gmms}
#Assign PCA Values to Observations
dims <- get_pca_ind(duo_pca)
duodata_pca <- duodata |> 
  mutate(pca_dim1 = dims$coord[,1],
         pca_dim2 = dims$coord[,2],
         pca_dim3 = dims$coord[,3],
         pca_dim4 = dims$coord[,4])

#Build GMM with PCA
gmm_pca <- Mclust(data = dplyr::select(duodata_pca, starts_with("pca_dim")))
summary(gmm_pca)

#Build GMM withOUT PCA - We just get 1 cluster!
gmm_no_pca <- Mclust(data = dplyr::select(duodata_pca, -user, -starts_with("pca_dim")))
summary(gmm_no_pca)
```

```{r assign-clusters}
#Attach clusters to data
gmm_preds <- predict(gmm_pca)
duodata_pca_gmm <- bind_cols(duodata_pca, gmm_preds$classification) 

#Rename gmm_cluster column
duodata_pca_gmm <- duodata_pca_gmm |> 
  rename(gmm_cluster = ...32)
```

```{r viz-pca-dims-by-clusters}
duodata_pca_gmm |> 
  mutate(gmm_cluster = as.factor(gmm_cluster)) |> 
  ggplot(aes(x = pca_dim1, 
             y = pca_dim2)) + 
  geom_point(aes(color = gmm_cluster)) + 
  geom_vline(xintercept = 0) + 
  geom_hline(yintercept = 0) + 
  labs(title = 'Duolingo User Clusters by PCA Dimension', 
       subtitle = 'Front View', 
       x = 'PCA Dimension 1', 
       y = 'PCA Dimension 2') + 
  theme_minimal() +
  theme(legend.position = 'bottom')

duodata_pca_gmm |> 
  mutate(gmm_cluster = as.factor(gmm_cluster)) |> 
  ggplot(aes(x = pca_dim1, 
             y = pca_dim3)) + 
  geom_point(aes(color = gmm_cluster)) + 
  geom_vline(xintercept = 0) + 
  geom_hline(yintercept = 0) + 
  labs(title = 'Duolingo User Clusters by PCA Dimension',
       subtitle = 'Bottom View', 
       x = 'PCA Dimension 1', 
       y = 'PCA Dimension 3') + 
  theme_minimal() +
  theme(legend.position = 'bottom')

duodata_pca_gmm |> 
  mutate(gmm_cluster = as.factor(gmm_cluster)) |> 
  ggplot(aes(x = pca_dim3, 
             y = pca_dim2)) + 
  geom_point(aes(color = gmm_cluster)) + 
  geom_vline(xintercept = 0) + 
  geom_hline(yintercept = 0) + 
  labs(title = 'Duolingo User Clusters by PCA Dimension', 
       subtitle = 'Side View', 
       x = 'PCA Dimension 1', 
       y = 'PCA Dimension 3') + 
  theme_minimal() +
  theme(legend.position = 'bottom')
```

```{r pca-3d}
mycolors <- c('red', 'gold', 'forestgreen', 'royalblue3', 'skyblue3', 'purple', 'hotpink')
duodata_pca_gmm$color <- mycolors[ as.numeric(duodata_pca_gmm$gmm_cluster) ]

#Plot the 3rd PCA dimension, with labels (little harder to read)
plot3d(x = duodata_pca_gmm$pca_dim1, 
       y = duodata_pca_gmm$pca_dim2, 
       z = duodata_pca_gmm$pca_dim3,
       col = duodata_pca_gmm$color, 
       #type = 's', 
       #radius = .15, 
       xlab = 'PCA Dim 1', 
       ylab = 'PCA Dim 2', 
       zlab = 'PCA Dim 3', 
       box = FALSE)
```

```{r examine-var-importance}
#PCA Variable Averages
cluster_averages <- duodata_pca_gmm |>
  dplyr::select(gmm_cluster, starts_with("pca_dim")) |> 
  group_by(gmm_cluster) |>
  summarize(across(where(is.numeric), mean)) |>
  ungroup() |>
  pivot_longer(-gmm_cluster, 
               names_to = "engagement_metrics", 
               values_to = "value")

ggplot(cluster_averages, 
       aes(x = engagement_metrics, 
           y = value, 
           fill = engagement_metrics)) + 
  geom_col() +
  geom_text(aes(label = round(value, 2)), 
            position = position_dodge(width = 0.9), 
            hjust = -0.2, vjust = 0.5) +
  coord_flip() +
  facet_wrap(~ gmm_cluster) +
  labs(x = "Engagement Metric", 
       y = "Value", 
       title = "Engagement Metrics by Cluster") +
  theme(legend.position = "none")



#Non-PCA Variables Average
cluster_averages <- duodata_pca_gmm |>
  dplyr::select(-starts_with("pca_dim")) |>
  group_by(gmm_cluster) |>
  summarize(across(where(is.numeric), mean)) |>
  ungroup() |>
  pivot_longer(-gmm_cluster, 
               names_to = "engagement_metrics", 
               values_to = "value")

ggplot(cluster_averages, aes(x = engagement_metrics, 
                             y = value, 
                             fill = engagement_metrics)) + 
  geom_col() +
  geom_text(aes(label = round(value, 2)), 
            position = position_dodge(width = 0.9), 
            hjust = -0.2, vjust = 0.5) +
  coord_flip() +
  facet_grid(~ gmm_cluster) +
  labs(x = "Engagement Metric", 
       y = "Value", 
       title = "Engagement Metrics by Cluster") +
  theme(legend.position = "none")
```

# Build Predictive Model

## Split Data

```{r split-data}
#NOTE: Data already standardized
set.seed(1234) 
train <- sample_frac(duodata_pca_gmm, 0.8)
test <- filter(duodata_pca_gmm, !duodata_pca_gmm$user %in% train$user) 
```

```{r build-boost-rf}
trControl = trainControl(method = "cv",
                         number = 10)

#Build without PCA Dimensions and without Clusters
# train <- train |> dplyr::select(-user, 
#                                 -starts_with("pca_dim"), 
#                                 -gmm_cluster)
# test <- test |> dplyr::select(-user, 
#                               -starts_with("pca_dim"), 
#                               -gmm_cluster)

#Build with ONLY PCA
# train <- train |> dplyr::select(drop_day, 
#                                 starts_with("pca_dim"))
# test <- test |> dplyr::select(drop_day, 
#                               starts_with("pca_dim"))

#Build with ONLY GMM Cluster
# train <- train |> dplyr::select(drop_day, 
#                                 gmm_cluster)
# test <- test |> dplyr::select(drop_day, 
#                               gmm_cluster)

#Build with PCA AND GMM
train <- train |> dplyr::select(drop_day,
                                gmm_cluster,
                                starts_with("pca_dim"))
test <- test |> dplyr::select(drop_day,
                              gmm_cluster,
                              starts_with("pca_dim"))

set.seed(1234)
m_boost_rf <- train(drop_day ~ .,
                   data = train, 
                   method = "xgbTree", 
                   trControl = trControl, 
                   verbose = FALSE, 
                   verbosity = 0)
```

```{r info-boost-rf}
plot(m_boost_rf)
varImp(m_boost_rf)
```

```{r predict-rf}
predicted_drop_train_rf <- predict(m_boost_rf, train)
predicted_drop_test_rf <- predict(m_boost_rf, test)

train_predictions <- tibble(predicted_drop = predicted_drop_train_rf,
                            drop = train$drop_day, 
                            source = 'train')

test_predictions <- tibble(predicted_drop = predicted_drop_test_rf,
                           drop = test$drop_day,
                           source = 'test')

all_predictions <- bind_rows(train_predictions, test_predictions)

ggplot(all_predictions, 
       aes(x = drop,
           y = predicted_drop, 
           color = source)) +
  geom_point() +
  geom_abline(aes(intercept = 0, 
                  slope = 1), 
              lty = 2) +
  theme_bw()
```

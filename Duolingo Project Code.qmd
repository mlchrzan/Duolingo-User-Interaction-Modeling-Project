---
title: "Duolingo Project Code"
author: "Michael Chrzan, Alexa Sparks, Tracy Li"
format: html
editor: visual
---

# Introduction

...

# Research Questions

...

# Setup

## Libraries

Which libraries will we use and why?

```{r libraries}
library(tidyverse)
library(skimr)
library(naniar)
library(gt)
library(MASS)
library(caret)
library(vip)
library(rpart.plot)
library(randomForest)
library(FactoMineR)
library(factoextra)
library(corrplot)
library(mclust)
library(DescTools)
```

## Import Data

```{r import-data}
duodata <- read_csv("data_flatformat.csv")
```

# Explore Data

```{r view-data}
#glimpse(duodata)
duoskim <- skim(duodata)
duoskim
```

## Missing Data

```{r viz-missing}
#Setting sample since these packaged functions throw errors for large data
set.seed(1234)
duosample <- sample_n(duodata, 2622956/54)

vis_miss(duosample, 
         show_perc = TRUE) 
miss_var_summary(duosample)

duosample_mis <- duosample |> 
  dplyr::select(prompt, time, client, format, session)

gg_miss_var(duosample_mis)
gg_miss_var(duosample_mis, 
            facet = client, 
            show_pct = TRUE)
gg_miss_var(duosample_mis, 
            facet = format, 
            show_pct = TRUE)
gg_miss_var(duosample_mis, 
            facet = session, 
            show_pct = TRUE)
gg_miss_upset(duosample_mis)

```

```{r examine-mis-prompt}
#Examining Missiningness Related to Prompt and Time with Other Variables #Is all the missing prompt data in format? Appears so (NOTE, not the sample now, full dataset) 
#Prompt 
duodata |> 
  dplyr::select(prompt, format, session, client) |> 
  group_by(format, session, client) |> 
  miss_var_summary() |> 
  arrange(desc(pct_miss)) 

#Time 
duodata |> 
  dplyr::select(time, format, session, client) |> 
  group_by(format, session, client) |> 
  miss_var_summary() |> 
  arrange(desc(pct_miss))
```

```{r handle-missing}

#JUST GOING TO REMOVE PROMPT
# duodata |> 
#   group_by(exercise_id) |> 
#   mutate(exercise_accuracy = mean(label)) |> ungroup() |> 
#   filter(format == 'listen') |>
#   filter(exercise_accuracy == 1) |>
#   mutate(prompt = if_else(is.na(prompt), 'identifiable', prompt)) |> 
#   dplyr::select(-instance_id,
#                 -part_of_speech,
#                 -contains('dependency'), -countries) 

#If we can get the exact prompt, we can match it to users who got it wrong

#Could we also just remove prompt? Prompt missing data is entirely on the listen format and probably wouldn't help with predictions. 

#Time missing data seems to mostly be a problem on iOS but Michael can't see a pattern beyond that. 
```

# Data Cleaning

```{r remove-mis-remaining}
duodata <- duodata |> 
  dplyr::select(-prompt) |> 
  na.omit()
```

```{r viz-variable-dist}
duodata |> 
  pivot_longer(cols = c(part_of_speech, client, session, format), 
               names_to = "variable", 
               values_to = "value") |> 
  ggplot() + geom_bar(aes(value)) + 
  facet_wrap(. ~ variable, scales = "free") + 
  coord_flip() 

duodata |> 
  pivot_longer(cols = c(dependency_label, countries), 
               names_to = "variable", 
               values_to = "value") |> 
  ggplot() + 
  geom_bar(aes(value)) + 
  facet_wrap(. ~ variable, scales = "free") + 
  coord_flip() 

duodata |> 
  pivot_longer(cols = where(is.numeric), 
               names_to = "variable", 
               values_to = "value") |> 
  ggplot() + 
  geom_histogram(aes(value)) + 
  facet_wrap(. ~ variable, scales = "free") 

#Should we include folks at 0 days?

```

```{r check-data}
distinct(duodata, countries) 

#Checking Time Values 
summary(duodata$time)
duodata |> 
  ggplot(aes(x = time)) + 
  geom_histogram()

  #Checking average user time
duodata |> 
  group_by(user) |> 
  mutate(avg_time = mean(time)) |> 
  dplyr::select(user, avg_time, time) |> 
  arrange(desc(time))
      #QTNA: Why don't some users have an avg_time but have time? #Number of Users with ONLY        less than 1 day, 52 - resulting in ~30000 rows 

  #This shows that 60 users used the app for longer than 14 hours in one session
duodata |> 
  group_by(user) |> 
  summarize(max_time = max(time)) |> 
  arrange(desc(max_time)) |> 
  filter(max_time < 50400)


#Large Dependency Edge Head Value (originally 442 rows larger than 10)
summary(duodata$dependency_edge_head)
duodata |> 
  ggplot(aes(x = dependency_edge_head)) + 
  geom_histogram()



#Users who used the app less than 1 day
one_days <- duodata |> 
  group_by(user) |> 
  mutate(max_days = max(days)) |> 
  ungroup() |> 
  filter(max_days < 1) 

n_distinct(one_days$user) 
#Curious if there's any patterns in how these folks use it rather than others. 
#Michael noticed from the histogram that users trail off as days increases.

#Proportion of Correct Responses, 14.62% 
sum(duodata$label == 1)/sum(duodata$label == 0)

```

```{r clean-data}
#NOT SURE REPLACEMENT MAKES SENSE HERE OR REMOVAL, DO WE JUST LEAVE IT?? 
#Dependency edge heads
# duodata$dependency_edge_head <- Winsorize(duodata$dependency_edge_head, 
#                                           minval = 0,
#                                           maxval = 7, 
#                                           na.rm = TRUE)

#Substituting extreme values with seemingly reasonable maximums
  
  #Removing instances with logging issues (time < 1) as noted in the documentation
duodata <- duodata |> 
  filter(time > 1)
  #Replacing users who used the app longer than 14 hours with 14 hours as their max
duodata$time <- Winsorize(duodata$time,
                          maxval = 50400, 
                          na.rm = TRUE)
```

```{r variable-creation}
#AVERAGES

#Averge Accuracy, Session Time, and Days for each User for their entire time 
duodata <- duodata |> 
  group_by(user) |> 
  mutate(avg_accuracy_user = mean(label), 
         avg_session_time_user = mean(time), 
         avg_days_user = mean(days)) |> 
  ungroup()
  #dplyr::select(user, session_id, exercise_id, contains('avg')) 

#Averge Session Accuracy per User and Session Time per Day 

#Is the day variable aligned for each user? i.e. If I skipped 2 days, is my day 2 the same as a continuous user's day 4? Right now I am assuming yes for all 

duodata <- duodata |> 
  mutate(days_count = round(days)) |> 
  group_by(user, days_count) |> 
  mutate(avg_daily_accuracy_user = mean(label), 
         avg_daily_session_time_user = mean(time)) |> 
  ungroup() 
  #dplyr::select(user, days, session_id, exercise_id, contains('avg')) 


#Averge Number of Exercises per Session and Number of Exercises by Session_id 
duodata <- duodata |> 
  group_by(session_id) |> 
  mutate(num_exercises_this_session = n_distinct(exercise_index)) |> 
  ungroup() |> 
  group_by(user) |>
  mutate(avg_exercises_per_session = mean(num_exercises_this_session)) |>
  ungroup() 
  #dplyr::select(user, session_id, num_session_exercises, avg_exer_per_session) 


#Averge Number of Exercises per Day and Number of Exercises by Session_id 
duodata <- duodata |> 
  group_by(session_id) |> 
  mutate(num_exercises_this_session = n_distinct(exercise_index)) |> 
  ungroup() |> 
  mutate(days_count = round(days)) |> 
  group_by(user, days_count) |> 
  mutate(avg_daily_num_exercise = mean(num_exercises_this_session)) 
  #dplyr::select(user, exercise_id, days_count, num_session_exercises, avg_daily_num_exercise) 

#Averge Number of Sessions per Day and Number of Sessions By Day 
duodata <- duodata |> 
  mutate(days_count = round(days)) |> 
  group_by(user, days_count) |> 
  mutate(num_sessions_this_day = n_distinct(session_id)) |> 
  ungroup() |> 
  group_by(user) |> 
  mutate(avg_num_sessions = mean(num_sessions_this_day)) 
  #dplyr::select(user, days_count, session_id, num_sessions_this_day, avg_num_sessions) 


#Accuracy by Exercise and Average Accuracy by User 
duodata <- duodata |> 
  group_by(exercise_id) |> 
  mutate(exercise_accuracy = mean(label)) |> 
  ungroup() |> 
  group_by(user) |> 
  mutate(avg_exercise_accuracy = mean(exercise_accuracy)) 
  #dplyr::select(user, exercise_id, exercise_accuracy, avg_exercise_accuracy) 


#TOTALS


#Total Number of of Countries by User 
duodata <- duodata |> 
  group_by(user) |> 
  mutate(country_count = str_count(countries, ",") + 1) |> 
  ungroup() 
  #dplyr::select(countries, country_count) |> #arrange(desc(country_count)) 


#Total Time Spent on App by User
user_times <- duodata |> 
  group_by(user, exercise_id) |> 
  summarize(total_time_exer = sum(time), 
            .groups = 'keep') |> 
  ungroup() |> 
  group_by(user) |> 
  summarise(total_time = mean(total_time_exer), 
            .groups = 'keep')

duodata <- left_join(duodata, user_times, by = join_by(user))
rm(user_times)


#Total Number of Exercises per User
tot_exer <- duodata |> 
  group_by(user) |> 
  summarize(total_exercises = n_distinct(exercise_id)) |> 
  ungroup()

duodata <- left_join(duodata, tot_exer, by = join_by(user))
rm(tot_exer)


#Total Number of Sessions per User
tot_sess <-  duodata |> 
  group_by(user) |> 
  summarize(total_sessions = n_distinct(session_id)) |> 
  ungroup()

duodata <- left_join(duodata, tot_sess, by = join_by(user))
rm(tot_sess)


#Total Number of Session by Type per User
  #Test
n_test <- duodata |> 
  filter(session == 'test') |> 
  group_by(user) |>
  summarize(total_tests_user = n_distinct(session_id)) |> 
  ungroup()

duodata <- left_join(duodata, n_test, by = join_by(user))
rm(n_test)

  #Practice
n_practice <- duodata |> 
  filter(session == 'practice') |> 
  group_by(user) |>
  summarize(total_practice_user = n_distinct(session_id)) |> 
  ungroup()

duodata <- left_join(duodata, n_practice, by = join_by(user))
rm(n_practice)

  #Lesson
n_lesson <- duodata |> 
  filter(session == 'lesson') |> 
  group_by(user) |>
  summarize(total_lesson_user = n_distinct(session_id)) |> 
  ungroup()

duodata <- left_join(duodata, n_lesson, by = join_by(user))
rm(n_lesson)


#Number of Sessions by Type by Day per User
  #Test
test_day_user <- duodata |> 
  mutate(days_round = round(days)) |> 
  filter(session == 'test') |> 
  group_by(user, days_round) |> 
  summarize(total_tests_day_user = n_distinct(session_id), 
            .groups = 'keep') |> 
  ungroup() 

duodata <- left_join(duodata |> mutate(days_round = round(days)), 
                     test_day_user, 
                     by = join_by(user, days_round)) |> 
  mutate(total_tests_day_user = if_else(is.na(total_tests_day_user), 
                                        0, 
                                        total_tests_day_user)) |> 
  dplyr::select(-days_round) 

rm(test_day_user)

  #Practice
prac_day_user <- duodata |> 
  mutate(days_round = round(days)) |> 
  filter(session == 'practice') |> 
  group_by(user, days_round) |> 
  summarize(total_prac_day_user = n_distinct(session_id), 
            .groups = 'keep') |> 
  ungroup() 

left_join(duodata |> mutate(days_round = round(days)), 
          prac_day_user, 
          by = join_by(user, days_round)) |> 
  mutate(total_prac_day_user = if_else(is.na(total_prac_day_user), 
                                       0, 
                                       total_prac_day_user)) |> 
  dplyr::select(-days_round) 

rm(prac_day_user)

  #Lesson
lesson_day_user <- duodata |> 
  mutate(days_round = round(days)) |> 
  filter(session == 'lesson') |> 
  group_by(user, days_round) |> 
  summarize(total_lesson_day_user = n_distinct(session_id), 
            .groups = 'keep') |> 
  ungroup() 

left_join(duodata |> mutate(days_round = round(days)), 
          lesson_day_user, 
          by = join_by(user, days_round)) |> 
  mutate(total_lesson_day_user = if_else(is.na(total_lesson_day_user), 
                                       0, 
                                       total_lesson_day_user)) |> 
  dplyr::select(-days_round) 

rm(lesson_day_user)


#Number of Session by Type for ALL Users
tot_sess_type <- duodata |> 
  group_by(session) |> 
  summarize(total_session_type = n_distinct(session_id),
            .groups = 'keep') |> 
  ungroup()

duodata <- left_join(duodata, tot_sess_type, join_by(session))
  
rm(tot_sess_type)


#Total Number of Session by Type by Day for ALL Users
tot_sess_t_d <- duodata |> 
  mutate(days_round = round(days)) |> 
  group_by(session, days_round) |> 
  summarize(total_session_type_day = n_distinct(session_id), 
            .groups = 'keep') |> 
  ungroup()

duodata <- left_join(duodata |> mutate(days_round = round(days)), 
          tot_sess_t_d, 
          by = join_by(session, days_round)) |> 
  dplyr::select(-days_round)

rm(tot_sess_t_d)

  
#OTHER


#Get Number of Days Skipped by User
skipped <- duodata |>
  mutate(days_round_down = floor(days)) |> 
  group_by(user, days_round_down) |> 
  summarise(.groups = 'keep') |>
  ungroup() |> 
  mutate(days_skipped_user = days_round_down - dplyr::lag(days_round_down) - 1) |> 
  ungroup() |> 
  mutate(days_skipped_user = if_else(is.na(days_skipped_user) | days_skipped_user < 0, 
                                     0, 
                                     days_skipped_user))

duodata <- left_join(duodata |> mutate(days_round_down = floor(days)), 
          skipped, 
          by = join_by(user, days_round_down)) |> 
  dplyr::select(-days_round_down) 

rm(skipped)

#Get Last Day User Used App
duodata <- duodata |> 
  group_by(user) |> 
  mutate(drop_day = max(round(days))) 


#TIDY UP


#Removing Variables not included in our final analysis 

duodata <- duodata |> dplyr::select(-countries)

#Should we remove variables related to tokens-level info? Michael wasn't sure
#IF WE ARE, CODE BELOW SHOULD DO IT
# duodata <- duodata |> dplyr::select(-token, 
#                                     -part_of_speech, 
#                                     -dependency_label, 
#                                     -dependency_edge_head,
#                                     -token_index, 
#                                     -instance_id)

#Removing Duplicated Rows (each row is still based on the tokens, so there are duplicates)
#duodata <- distinct(duodata)

#Check New Unit of Analysis
# n_distinct(duodata$exercise_id)
# distinct(duodata)
```

```{r check-new-distributions}
duodata |> 
  pivot_longer(cols = where(is.numeric), 
               names_to = "variable", 
               values_to = "value") |> 
  ggplot() + 
  geom_histogram(aes(value)) + 
  facet_wrap(. ~ variable, scales = "free") 
```

# Examine Data

```{r}
duodata |> 
  group_by(user) |> 
  summarize(last_day = max(days)) |> 
  ggplot(aes(x = last_day)) + 
  geom_histogram(fill = "#7ac70c") + 
  labs(title = 'User Retention Over Time',
       subtitle = 'Number of Users Returning to Duolingo Each Day Decreases Over Time', 
       x = 'Days', 
       y = 'Number of Users Lost') +
  theme_minimal()

duodata |> 
  ggplot(aes(x = days)) + 
  geom_histogram(fill = "#7ac70c") + 
  labs(title = 'User Engagement Over Time',
       subtitle = 'Number of Words Translated on Duolingo Each Day Steadily Decreases', 
       x = 'Days', 
       y = 'Number of Activities') +
  scale_y_continuous(labels = ~ format(.x, scientific = FALSE)) +
  theme_minimal() 
```

```{r viz for seminar}
#Create a viz of x = time, y = accuracy measured by proportion of label that is 1
#Could also show if accuracy changes based on session

#SOMETHING VERY STRANGE HAPPENING HERE w/ iOS-------
duosample |> 
  group_by(user, session_id) |> 
  mutate(user_session_accuracy = mean(label)) |> 
  ungroup() |> 
  ggplot(aes(x = days, 
             y = user_session_accuracy, 
             color = session)) + 
  geom_smooth() + 
  facet_grid(~client) +
  labs(title = 'User Session Accuracy Over Time', 
       subtitle = '',
       x = 'Session Type', 
       y = 'Session Accuracy') + 
  theme_minimal() 
#--------------------------------------------------

duodata |> 
  group_by(session) |> 
  summarise(session_accuracy = mean(label)) |> 
  ungroup() |> 
  ggplot(aes(x = session, 
             y = session_accuracy, 
             fill = session)) + 
  geom_col() +
  scale_fill_manual(values = c('#7ac70c', '#7ac70c', '#7ac70c')) +
  labs(title = 'Practice Makes Perfect?', 
       subtitle = 'Average Session Accuracy based on Session Type',
       x = 'Session Type', 
       y = 'Session Accuracy') + 
  theme_minimal() +
  theme(legend.position = 'none') +
  coord_flip()

duodata |> 
  group_by(session_id) |> 
  mutate(session_accuracy = mean(label)) |> 
  ungroup() |> 
  ggplot(aes(x = days, 
             y = session_accuracy, 
             color = session)) + 
  geom_smooth() + 
  labs(title = 'It All Adds Up', 
       subtitle = 'Session Accuracy Over Time',
       x = 'Days Since Starting Duolingo', 
       y = 'Session Accuracy', 
       color = 'Session Type') + 
  theme_minimal() + 
  theme(legend.position = 'bottom')

duodata |> 
  group_by(session_id) |> 
  mutate(session_accuracy = mean(label)) |> 
  ungroup() |> 
  ggplot(aes(x = days, 
             y = session_accuracy)) + 
  geom_smooth() + 
  labs(title = 'It All Adds Up', 
       subtitle = 'Session Accuracy Over Time',
       x = 'Days Since Starting Duolingo', 
       y = 'Session Accuracy', 
       color = 'Session Type') + 
  theme_minimal() + 
  theme(legend.position = 'bottom')


#Test Outs------------------------------------------------------
duosample |> 
  group_by(user) |> 
  mutate(user_accuracy = sum(label == 1)/sum(!is.na(label))) |> 
  ungroup() |> 
  ggplot(aes(x = user_accuracy)) +
  geom_histogram() + 
  labs(title = 'Distribution of User Accuracy')


duosample |> 
  group_by(user, exercise_id) |> 
  mutate(user_exer_accuracy = mean(label)) |> 
  ungroup() |> 
  ggplot(aes(x = days, 
             y = user_exer_accuracy)) +
  geom_smooth() + 
  labs(title = 'User Exercise Accuracy Over Time')

duosample |> 
  group_by(session_id) |> 
  mutate(avg_session_accuracy = mean(label), 
         avg_session_time = mean(time)) |> 
  ungroup() |> 
  ggplot(aes(x = avg_session_time, 
             y = avg_session_accuracy, 
             color = session)) + 
  geom_smooth() + 
  labs(title = 'Session Accuracy vs Session Length', 
       subtitle = '',
       x = 'Average Time for the Session', 
       y = 'Average Session Accuracy', 
       color = 'Session Type') + 
  theme_minimal() +
  theme(legend.position = 'bottom')

#What parts of speech do people most get wrong??
```

```{r pca}
duo_pca <- PCA(dplyr::select(duodata, where(is.numeric)))
```

```{r examine-PCA}
get_eigenvalue(duo_pca)
fviz_eig(duo_pca, addlabels = TRUE)

var <- get_pca_var(duo_pca)
corrplot(var$cor, tl.col = "black", method = "color")

var$cos2
corrplot(var$cos2, 
         is.corr = FALSE, 
         tl.col = "black", 
         method = "color")
```

# Create Clusters

```{r hierarchical}
duodata_clust <- duodata |> 
  dplyr::select(-part_of_speech, -dependency_label, -dependency_edge_head, -exercise_index, -token_index, -token, - label)

duo_diff <- dist(duodata_clust, method = "euclidean")
duo_hc_c <- hclust(duo_diff, method = "complete")

duo_dend <- as.dendrogram(duo_hc_c)|> 
  color_labels(k = 3)  |> 
  color_branches(k = 3)

duo_dend |> 
  set("labels_cex", 0.5) |> 
  plot(main = "Complete")
```

```{r Gaussian}
#library(mclust)
#How do we make it cluster by user not by instance??
duodata |> 
  group_by(user) |> 
  summarize()


set.seed(1234)
gmm_1 <- Mclust(data = duodata, 
                G = 4)
summary(gmm_1)

gmm_preds <- predict(gmm_1)

duodata_gmm <- bind_cols(duodata, gmm_preds$classification) |> 
  rename(gmm_cluster = ...31)

duodata_gmm |> 
  dplyr::select(instance_id, user, gmm_cluster) |> 
  summarize(avg_clust = median(gmm_cluster))

# #To assign back to the data
# gmm_preds <- predict(gmm_1)
# 
# d1_gmm <- bind_cols(d1, gmm_preds$z) |> 
#   mutate(.cluster = gmm_preds$classification)
# 
# d1_gmm
```

# Build Model

## Split Data

```{r split-data-on-user}
set.seed(1234)

unique_users <- unique(duodata$user)

# Randomly split unique users into training and testing sets
train_users <- sample(unique_users, size = floor(0.8 * length(unique_users)))

# Create traiing and testing sets based on user split
train_by_users <- duodata |> filter(user %in% train_users)
test_by_users <- duodata |> filter(!(user %in% train_users))
```

```{r split-data}
set.seed(1234) 
train <- sample_frac(duodata, 0.8)
test <- filter(duodata, !duodata$instance_id %in% train$instance_id) 
```

```{r build-rf}
m_rf <- randomForest(as_factor(label) ~ .
                     - exercise_accuracy
                     - instance_id 
                     - exercise_id
                     - session_id,
                     data = train, 
                     ntree = 100,
                     mtry = 5)

m_rf
```

```{r info-rf}
vip(m_rf)

#Error Rates
error_rates <- as_tibble(m_rf$err.rate)
error_rates <- error_rates |>
  mutate(Tree = 1:nrow(error_rates)) |>
  pivot_longer(cols = -Tree, names_to = "Type", values_to = "Error")
ggplot(error_rates, aes(x = Tree, 
                        y = Error, 
                        color = Type)) + 
  geom_line() +
  theme_bw()

plot(m_rf)
```

```{r predict-rf}
#Train
predicted_label <- predict(m_rf, train)
confusion_rf <- table(predicted_label, train$label)
confusion_rf
accuracy_rf <- sum(diag(confusion_rf)) / nrow(train)
accuracy_rf

#Test
predicted_label <- predict(m_rf, test)
confusion_rf <- table(predicted_label, test$label)
confusion_rf
accuracy_rf <- sum(diag(confusion_rf)) / nrow(test)
accuracy_rf
```

```{r build-boost-rf}
trControl = trainControl(method = "cv", number = 5)
set.seed(1234)
m_boost_rf <- train(as.factor(label) ~ . 
                    - token
                    - dependency_label 
                    - dependency_edge_head
                    - exercise_accuracy 
                    - token_index
                    - days_count 
                    - part_of_speech 
                    - user
                     - instance_id 
                     - exercise_id
                     - session_id, 
                   data = train, 
                   method = "xgbTree", 
                   trControl = trControl, 
                   verbose = FALSE, 
                   verbosity = 0)
```

```{r info-boost-rf}
summary(m_boost_rf)
plot(m_boost_rf)
print(m_boost_rf)
varImp(m_boost_rf)
```

```{r predict-rf}
#Train
predicted_label <- predict(m_boost_rf, train)
confusion_boost_rf <- table(predicted_label, train$label)
confusion_boost_rf
accuracy_boost_rf <- sum(diag(confusion_rf)) / nrow(train)
accuracy_boost_rf

#Test
predicted_label <- predict(m_boost_rf, test)
confusion_boost_rf <- table(predicted_label, test$label)
confusion_boost_rf
accuracy_boost_rf <- sum(diag(confusion_rf)) / nrow(test)
accuracy_boost_rf
```
